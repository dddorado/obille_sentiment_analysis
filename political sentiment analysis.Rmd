---
title: "Political Text Analysis"
output: html_notebook
---

# Loading the required packages
```{r}
library(tidyverse)
library(readxl)
library(tidytext)
library(tm)
library(topicmodels)
```

# Loading the dataset
```{r}
dataset <- read_xlsx('RapplerMarch27-25results.xlsx')
glimpse(dataset)
```

# Wrangling the appropriate data types and row number
```{r}
dataset$type <- as.factor(dataset$type)
dataset$commentDate <- as.factor(dataset$commentDate)
dataset <- dataset %>%
  mutate(id = row_number())
```

# Wrangling Text: Facebook post and comments

## Counting words
```{r}
dataset %>% 
  unnest_tokens(word, comment) %>%
  count(word) %>% 
  arrange(desc(n)) %>%
  head()
```

## Filtering stopwords
```{r}
dataset %>% 
  unnest_tokens(word, comment) %>% 
  anti_join(stop_words) %>%
  count(word) %>% 
  arrange(desc(n)) %>%
  head()
```

## Creating custom stop words

```{r}
custom_stop_words <- tribble(
  ~word, ~lexicon,
  "na", "CUSTOM",
  "sa", "CUSTOM",
  "ng", "CUSTOM",
  "po", "CUSTOM",
  "ang", "CUSTOM",
  "yung", "CUSTOM",
  "lang", "CUSTOM",
  "mga", "CUSTOM",
  "ko", "CUSTOM",
  "para", "CUSTOM",
  "kung", "CUSTOM",
  "ako", "CUSTOM",
  "pa", "CUSTOM",
  "kasi", "CUSTOM",
  "ka", "CUSTOM",
  "din", "CUSTOM",
  "mo", "CUSTOM",
  "niyo", "CUSTOM",
  "di", "CUSTOM",
  "kayo", "CUSTOM",
  "pero", "CUSTOM",
  "naman", "CUSTOM",
  "ba", "CUSTOM",
  "pag", "CUSTOM",
  "ano", "CUSTOM",
  "mag", "CUSTOM",
  "siya", "CUSTOM",
  "nyo", "CUSTOM",
  "mas", "CUSTOM",
  "rin", "CUSTOM",
  "namin", "CUSTOM",
  "yan", "CUSTOM",
  "sya", "CUSTOM",
  "https", "CUSTOM",
  "sila", "CUSTOM",
  "kapag", "CUSTOM",
  "kahit", "CUSTOM",
  "kaya", "CUSTOM",
  "pang", "CUSTOM",
  "nila", "CUSTOM",
  "yun", "CUSTOM",
  "ung", "CUSTOM",
  "tas", "CUSTOM",
  "si", "CUSTOM",
  "ay", "CUSTOM",
  "lng", "CUSTOM",
  "kay", "CUSTOM",
  "nya", "CUSTOM",
  "pag", "CUSTOM",
  "ito", "CUSTOM",
  "kami", "CUSTOM",
  "eh", "CUSTOM",
  "kau", "CUSTOM",
  "nang", "CUSTOM",
  "nman", "CUSTOM",
  "niya", "CUSTOM",
  "kau", "CUSTOM",
  "nga", "CUSTOM",
  "nag", "CUSTOM",
  "ikaw", "CUSTOM",
  "ni", "CUSTOM"
)

stop_words2 <- stop_words %>%
bind_rows(custom_stop_words)
```

## Final product: Tidy dataset
```{r}
# Tokenize the post_text dataset
tidy_dataset <- dataset %>% 
  # Tokenize the comment data
  unnest_tokens(word, comment) %>% 
  # Remove stop words
  anti_join(stop_words2)

tidy_dataset %>% 
  # Compute word counts and arrange in descending order
  count(word) %>% 
  arrange(desc(n))
```

# Visualizing word counts
```{r}
tidy_dataset %>%
count(word) %>%
filter(n > 50) %>%
arrange(desc(n)) %>%
ggplot( aes(x = word, y = n)) +geom_col() +  coord_flip() + ggtitle("Comment Word Counts")
```

## Reordering the plot
```{r}
tidy_dataset %>%
count(word) %>%
filter(n > 50) %>%
mutate(word2 = fct_reorder(word, n)) %>%
arrange(desc(n)) %>%
ggplot( aes(x = word2, y = n)) +geom_col() +  coord_flip() + ggtitle("Comment Word Counts")
```

# Plotting word clouds
```{r}
word_counts <- tidy_dataset %>%
count(word)

wordcloud(
  words = word_counts$word,
  freq = word_counts$n,
max.words = 30
)
```

# Measuring word proximity

## Create a function to clean corpus
```{r}
clean_corpus <- function(corpus) {
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, removeNumbers)
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "na", "sa", "ng", "po", "ang", "yung", "lang", "mga", "ko", "para", "kung", "ako", "pa", "kasi", "ka", "din", "mo", "niyo", "di", "kayo", "pero", "naman", "ba", "pag", "ano", "mag", "siya", "nyo", "mas", "rin", "namin", "yan", "sya", "https", "sila", "kapag", "kahit", "kaya", "pang", "nila", "yun", "ung", "tas", "si", "ay", "lng", "kay", "nya", "pag", "ito", "kami", "eh", "kau", "nang", "nman", "niya", "kau", "nga", "nag", "ikaw", "ni"))
    corpus <- tm_map(corpus, stripWhitespace)
    return(corpus)
}
```

## Generate Dendogram 
```{r, fig.height=10}
# Create a text corpus
factor.corpus <- dataset$comment %>%
VectorSource() %>%
VCorpus()
# Clean the created corpus
factor.corpus <- clean_corpus(factor.corpus)
# Create a dendogram
factor.corpus %>%
TermDocumentMatrix() %>%
removeSparseTerms(sparse = 0.99) %>%
as.matrix() %>%
dist() %>%
hclust() %>%
plot()
```

